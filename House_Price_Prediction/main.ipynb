{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\"\"\"Our dataset is boston city in Massachusetts famously know for boston tea party\n",
    "    With the help of data you have to predict the price of house in Boston\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(boston) #type is sklearn bunch => dict keys & values\n",
    "\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the key:DESCR values\n",
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.data)\n",
    "\n",
    "# Since data is nested loop understand it and try to find first element\n",
    "# print(boston.data[0]) \n",
    "# This gives single list having 13 features values we total have 506 such list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This to check price our goal is to find price i.e. target here to similar to our calculated \n",
    "print(boston.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.feature_names)\n",
    "boston.feature_names[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset \n",
    "- It's the 3rd step in 7 ML steps, sometimes it's also reffered as EDA **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(boston.data)\n",
    "\n",
    "#Check what is DataFrame and various method's of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "\n",
    "#Gives only first 5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.Can we change our column number to their names? \n",
    "# A. Our DataFrame has parameter column in which we can pass our required argument to set as column name\n",
    "\n",
    "dataset = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. Can we add output dependent variable in dataframe?\n",
    "# A. You can it's same as adding key with it's values but size should same\n",
    "\n",
    "dataset['Price'] = boston.target\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()\n",
    "\n",
    "# This gives us clear understanding of each column and row are any null(blank) values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()\n",
    "\n",
    "# Finding out stats measures of our dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull()\n",
    "# dataset.isnull().sum()\n",
    "\n",
    "\"\"\"Whenever you get dataset check whether it's have missing value the info just give idea is there present null or not\n",
    "But to find exactly where null value is present you should use isnull() method \n",
    "dataset.isnull().sum() just helps in counting and displaying the null \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploratory Data Analysis:- ###\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "############\n",
    "Cor-relation When you are dealing with regression problem try to find correlation \n",
    "############          \n",
    "1. Between Independent features aka Multicollinearity: How & which independent features are inter-related with each other\n",
    "2. Between Independent and Output features: Which independent features are related with output and what factor\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dataset.corr()\n",
    "# Move cursor over corr() see it's parameter pearson is most powerful way to find correlation\n",
    "# Values ranges between -1,0,1 {-1: Highly -ve dependent, 0: No dependence, 1: Highly postively dependent}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are not able to properly figure out correlation we are going to plot various graph to get better vizualization\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since it's difficult to understand from graph we can create some individual graphs of Independent vs Dependent. Later you can try to find Multicollinearity i.e. releation between independent\n",
    "\n",
    "fig,axs = plt.subplots(4,4,figsize=(12, 9),layout=\"constrained\")\n",
    "for count,ax in enumerate(axs.flat):\n",
    "    if count<13:\n",
    "        ax.scatter(dataset[boston.feature_names[count]],dataset['Price'])\n",
    "        ax.set_xlabel(boston.feature_names[count])\n",
    "        ax.set_ylabel(\"Price\")\n",
    "    else:\n",
    "        break\n",
    "fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0,wspace=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since with the help of plot we are ablt to find independent vs dependent correlation i.e. RM & LSTAT has releation with Price.\n",
    "# We also have some seaborn features that help in visualization how a ideal regression pattern should happen between features.\n",
    "\n",
    "import seaborn as sns\n",
    "sns.regplot(x=\"RM\",y='Price',data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x=\"LSTAT\",y='Price',data=dataset)\n",
    "\n",
    "# This both plot show how is regression pattern between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "     Now we got idea about features that have high impact on dependent output!\n",
    "     You can start with finding best model for our problem\n",
    "\"\"\"\n",
    "\n",
    "X = dataset.iloc[:,:-1]\n",
    "y = dataset.iloc[:,-1]\n",
    "\n",
    "\"\"\" \n",
    "     iloc is integer location [---,----] row,column\n",
    "    Since we are creating model for input we will not include output in that   \n",
    "\"\"\"\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Training and testing data \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=3)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data i.e. feature scaling it's name as Standardize data which means normalize the range of independent variables or features of data\n",
    "# Standardize scaler just change the features to same range is 1 feature CRIM in 0-40 and RAD changes from 2-30 and TAX changes from 0-1000 scaler just bring all to same scale 0 to 1 or -1 to 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "# Here we not applied fit_transform because we don't want our model to learn from the test data also it's should unaware and unseen from it.\n",
    "# If we used fit_transform model finds knowledge from that data i.e. it will learn from testing data also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "\n",
    "# Q. Why we are normalizing the data in linear regression?\n",
    "# A. Since we are using gradient descent in linear regression it will help to find global minima, Thus by normalizing we are setting same scale range for all features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Buidling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercepts\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Eqn of line is y = mx + c  <Slope is coefficient,wegiht) <y-intercept is called intercept,bias>\n",
    "Weight(Coefficient) := In machine learning linear regression model have weight attach to each features | independent variables \n",
    "Bias(Intercept)     := Bias or intercept is point where linear line intercept at y-axis\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(regression.coef_)\n",
    "print(regression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On which parameters of regrssion class the model has trained is find out by\n",
    "\n",
    "regression.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction made by our model on test data\n",
    "\n",
    "reg_pred = regression.predict(X_test)\n",
    "reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking how our modelprediction is performing w.r.t y_test\n",
    "\n",
    "\"\"\"\n",
    "We plotting graph between truth values i.e. y_test for predicted values if that gives straight line means model perform well\n",
    "When x & y both are same you get straight line.\n",
    "\"\"\"\n",
    "\n",
    "plt.scatter(y_test,reg_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot between residuals in prediction\n",
    "# Residual = actual value — predicted value\n",
    "# e = y — ŷ\n",
    "\n",
    "\n",
    "residuals = y_test - reg_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the residuals\n",
    "\n",
    "\"\"\" \n",
    "Our data is normal distributed with some outilers after +10 in x-axis\n",
    "\"\"\"\n",
    "\n",
    "sns.displot(residuals,kind='kde')\n",
    "sns.displot(residuals,kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot wrt prediction and residuals\n",
    "\n",
    "\"\"\"\n",
    "This plot show our residuals i.e. our error are uniformly distributed \n",
    "\"\"\"\n",
    "plt.scatter(reg_pred,residuals)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measures | Evaluation metric in Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Absoulte Mean Error:-\",mean_absolute_error(y_test,reg_pred))\n",
    "print(\"Mean Square Error:-\",mean_squared_error(y_test,reg_pred))\n",
    "print(\"Root Mean Square Error:-\",np.sqrt(mean_squared_error(y_test,reg_pred)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rsquare and adjusted Rsquare\n",
    "\n",
    "- Formula\n",
    "\n",
    "**R^2 = 1 - SSR/SST**\n",
    "\n",
    "*R^2 = coefficient of determination SSR = sum of square of residuals SST = total sum of squares*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y_test,reg_pred)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted Rsquare\n",
    "\n",
    "- Formula\n",
    "\n",
    "**R2 = 1 - [(1-R^2)*(n-1)/(n-k-1)]**\n",
    "\n",
    "where:\n",
    "\n",
    "*R2 : The R2 of the model n: The number of observations k: The number of prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.data[0],\"Shpae:\",boston.data[0].shape)\n",
    "print(boston.data[0].reshape(1,-1),\"Shape:\",boston.data[0].reshape(1,-1).shape)\n",
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.data[0].shape)\n",
    "# We need to transform new prediction data so that it's calculate as per our standardize way\n",
    "\"\"\"\n",
    "Q. Why we reshape our single input\n",
    "A. Boston is our dataset with shape (506,13) means 1 single input is of shape (1,13) but when we retrive it is in single list format\n",
    "    i.e. (13,) a simple list containig 13 elements so we reshape it to make (1,13) 1 row with 13 columns\n",
    "\"\"\"\n",
    "\n",
    "print(boston.data[0].reshape(1,-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we transform the data to make it of same range\n",
    "\n",
    "print(scaler.transform(boston.data[0].reshape(1,-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we predicting only single input\n",
    "print(regression.predict(scaler.transform(boston.data[0].reshape(1,-1))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on user input\n",
    "- Read the commets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kajal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [80], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Note here we are testing on unseen data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Provide your testing input here which is simple array of size 13 for eg. it will look like user_input = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m user_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])  \u001b[39m#give input data inside [] brackets \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m scaler\u001b[39m.\u001b[39;49mtransform(user_input\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(regression\u001b[39m.\u001b[39mpredict(scaler\u001b[39m.\u001b[39mtransform(user_input\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))))\n",
      "File \u001b[1;32mc:\\Users\\Kajal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    972\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    974\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 975\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    976\u001b[0m     X,\n\u001b[0;32m    977\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    978\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    979\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    980\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    981\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    982\u001b[0m )\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\Kajal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\Kajal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:918\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    916\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    917\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[1;32m--> 918\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    919\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    921\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    922\u001b[0m         )\n\u001b[0;32m    924\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[0;32m    925\u001b[0m     array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(array, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "#Note here we are testing on unseen data\n",
    "# Provide your testing input here which is simple array of size 13 for eg. it will look like user_input = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "\n",
    "user_input = np.array([])  #give input data inside [] brackets \n",
    "scaler.transform(user_input.reshape(1,-1))\n",
    "print(regression.predict(scaler.transform(user_input.reshape(1,-1)))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d3283b7ede15a3ba02d29b57611833951a6474bfc2330b92af0513ee46fd488"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
